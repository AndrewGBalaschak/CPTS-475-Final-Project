{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Check if CUDA (GPU) is available and set the device accordingly.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# The rest of your code goes here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert IPv4 string to integer\n",
    "def ip_to_int(ip):\n",
    "    octets = ip.split('.')\n",
    "    if len(octets) != 4:\n",
    "        return None\n",
    "    try:\n",
    "        int_ip = (int(octets[0]) << 24) + (int(octets[1]) << 16) + (int(octets[2]) << 8) + int(octets[3])\n",
    "        return int_ip\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "# Function to convert int to bitmask\n",
    "def int_to_bits(num):\n",
    "    bits = bin(num)[2:]\n",
    "    # pad with zeros if needed\n",
    "    bits = bits.zfill(32)\n",
    "\n",
    "    # return \".\".join(str(bit) for bit in bits)\n",
    "    return [int(bit) for bit in bits]\n",
    "\n",
    "# Function to convert IPV4 address to bitmask\n",
    "def ip_to_bits(ip):\n",
    "    octets = ip.split('.')\n",
    "    if len(octets) != 4:\n",
    "        return None\n",
    "    int_ip = (int(octets[0]) << 24) + (int(octets[1]) << 16) + (int(octets[2]) << 8) + int(octets[3])\n",
    "    \n",
    "    # Convert to bits\n",
    "    bits = bin(int_ip)[2:]\n",
    "\n",
    "    # Pad with zeros if needed\n",
    "    bits = bits.zfill(32)\n",
    "\n",
    "    # return \".\".join(str(bit) for bit in bits)\n",
    "    return [int(bit) for bit in bits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "11111111111111111111111111111111\n",
      "[0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "110000110001101111111110000000\n"
     ]
    }
   ],
   "source": [
    "print(ip_to_bits('255.255.255.255'))\n",
    "print(bin(ip_to_int('255.255.255.255'))[2:])\n",
    "print(ip_to_bits('48.198.255.128'))\n",
    "print(bin(ip_to_int('48.198.255.128'))[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"data/NF-UQ-NIDS.csv\")\n",
    "\n",
    "# Convert IP address strings into bitmask\n",
    "data['IPV4_SRC_ADDR'] = data['IPV4_SRC_ADDR'].apply(ip_to_bits)\n",
    "data['IPV4_DST_ADDR'] = data['IPV4_DST_ADDR'].apply(ip_to_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN dropped: 2\n"
     ]
    }
   ],
   "source": [
    "# Drop any rows that have NaN values\n",
    "old_size = len(data)\n",
    "data = data.dropna(how='any')\n",
    "print('Entries with NaN dropped: ' + str(old_size-len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split IP columns into 32 separate columns, one for each bit of address\n",
    "data[['sIP31', 'sIP30', 'sIP29', 'sIP28', 'sIP27', 'sIP26', 'sIP25', 'sIP24', 'sIP23', 'sIP22', 'sIP21', 'sIP20', 'sIP19', 'sIP18', 'sIP17', 'sIP16', 'sIP15', 'sIP14', 'sIP13', 'sIP12', 'sIP11', 'sIP10', 'sIP9', 'sIP8', 'sIP7', 'sIP6', 'sIP5', 'sIP4', 'sIP3', 'sIP2', 'sIP1', 'sIP0']] = pd.DataFrame(data['IPV4_SRC_ADDR'].tolist()).astype('bool')\n",
    "data[['dIP31', 'dIP30', 'dIP29', 'dIP28', 'dIP27', 'dIP26', 'dIP25', 'dIP24', 'dIP23', 'dIP22', 'dIP21', 'dIP20', 'dIP19', 'dIP18', 'dIP17', 'dIP16', 'dIP15', 'dIP14', 'dIP13', 'dIP12', 'dIP11', 'dIP10', 'dIP9', 'dIP8', 'dIP7', 'dIP6', 'dIP5', 'dIP4', 'dIP3', 'dIP2', 'dIP1', 'dIP0']] = pd.DataFrame(data['IPV4_DST_ADDR'].tolist()).astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN dropped: 2\n"
     ]
    }
   ],
   "source": [
    "# Drop any rows that have NaN values, again\n",
    "old_size = len(data)\n",
    "data = data.dropna(how='any')\n",
    "print('Entries with NaN dropped: ' + str(old_size-len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and labels (y)\n",
    "X = data.drop({'Label', 'Attack', 'Dataset', 'IPV4_SRC_ADDR', 'IPV4_DST_ADDR'}, axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Delete data so it can be garbage collected\n",
    "del data\n",
    "\n",
    "# Drop column labels\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
    "\n",
    "# Delete data so it can be garbage collected\n",
    "del X\n",
    "del y\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "\n",
    "# Model Definition\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))  # Sigmoid activation for binary classification\n",
    "        return x\n",
    "\n",
    "model = Classifier(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Optimizer Definition\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 100\n",
    "\n",
    "# Lists to store the loss values for training and validation sets\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_outputs = model(X_train).squeeze()\n",
    "    train_loss = criterion(train_outputs, y_train)\n",
    "    train_loss_list.append(train_loss.item())\n",
    "\n",
    "    test_outputs = model(X_test).squeeze()\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    test_loss_list.append(test_loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    if(((epoch+1) % 10) == 0):\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "del train_outputs\n",
    "del test_outputs\n",
    "del train_loss\n",
    "del test_loss\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(train_loss_list, label=\"train\")\n",
    "plt.plot(test_loss_list, label=\"validation\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predictions = test_outputs.max(1)\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(y_test.cpu(), predictions.cpu())\n",
    "    confusion = confusion_matrix(y_test.cpu(), predictions.cpu())\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print(f'Confusion Matrix:\\n{confusion}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
